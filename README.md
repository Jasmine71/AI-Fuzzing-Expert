## Deploy a model with vLLM
1. Install vLLM with `pip install vllm`
2. Follow this [link](https://github.com/QwenLM/Qwen2.5?tab=readme-ov-file#vllm) to deploy and serve a model locally
3. Follow this [link](https://github.com/QwenLM/Qwen-Agent) to customize an agent to query the model
